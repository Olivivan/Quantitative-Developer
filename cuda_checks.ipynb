{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94430b8f",
   "metadata": {},
   "source": [
    "#### Useful checks you can run for Cuda GPU Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute capability: (8, 6)\n",
      "['L2_cache_size', 'gcnArchName', 'is_integrated', 'is_multi_gpu_board', 'major', 'max_threads_per_multi_processor', 'minor', 'multi_processor_count', 'name', 'pci_bus_id', 'pci_device_id', 'pci_domain_id', 'regs_per_multiprocessor', 'shared_memory_per_block', 'shared_memory_per_block_optin', 'shared_memory_per_multiprocessor', 'total_memory', 'uuid', 'warp_size']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# List properties\n",
    "props = torch.cuda.get_device_properties(0)\n",
    "print(sorted([a for a in dir(props) if not a.startswith('_')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4f7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute capability, major and minor: (8, 6)\n",
      "Name: NVIDIA GeForce RTX 3090\n",
      "Total memory (GB): 23.99951171875\n",
      "Multi-processor count: 82\n",
      "Max threads per multi-processor: 1536\n",
      "Max threads per block (approx): n/a\n",
      "CUDA available: True\n",
      "torch.version.cuda: 12.6\n",
      "AMP available (autocast): True\n"
     ]
    }
   ],
   "source": [
    "# Full properties details\n",
    "print(\"Compute capability, major and minor:\", torch.cuda.get_device_capability(0))\n",
    "print(\"Name:\", props.name)\n",
    "print(\"Total memory (GB):\", props.total_memory / 1024**3)\n",
    "print(\"Multi-processor count:\", props.multi_processor_count)\n",
    "print(\"Max threads per multi-processor:\", getattr(props, \"max_threads_per_multi_processor\", \"n/a\"))\n",
    "print(\"Max threads per block (approx):\", getattr(props, \"max_threads_per_block\", \"n/a\"))\n",
    "\n",
    "# Verify mixed precision support\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"AMP available (autocast):\", hasattr(torch.cuda.amp, \"autocast\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0debf",
   "metadata": {},
   "source": [
    "#### Testing BFloat16 support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6c8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16 ops OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    x = torch.randn(4, device='cuda', dtype=torch.bfloat16)\n",
    "    y = x + x\n",
    "    print(\"bfloat16 ops OK\")\n",
    "except Exception as e:\n",
    "    print(\"bfloat16 ops failed:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
